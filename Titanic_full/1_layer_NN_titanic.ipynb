{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dataprocess(location):\n",
    "    df = pd.read_csv(location)\n",
    "    \n",
    "    df.drop_duplicates(inplace=True)    # inplace is for overwriting\n",
    "    \n",
    "    df = df.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\n",
    "    \n",
    "    # Use df.replace\n",
    "    df.Sex[df.Sex == 'male'] = 1\n",
    "    df.Sex[df.Sex == 'female'] = 0\n",
    "\n",
    "    df.Embarked[df.Embarked == 'S'] = 1\n",
    "    df.Embarked[df.Embarked == 'C'] = 0\n",
    "    df.Embarked[df.Embarked == 'Q'] = 2\n",
    "    \n",
    "    age = df['Age']\n",
    "    df['Age'] = (age - age.mean())/age.std()\n",
    "    df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "    fare = df['Fare']\n",
    "    df['Fare'] = (fare - fare.mean())/fare.std()\n",
    "    df['Fare'].fillna(df['Fare'].mean())\n",
    "\n",
    "    df.ffill(inplace = True) \n",
    "    print(df.isna().sum())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived    0\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "Pclass      0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# split into train test sets\n",
    "train_loc = r\"C:\\Users\\varun\\Coding\\ML\\titanic\\train.csv\"\n",
    "df_train = Dataprocess(train_loc)\n",
    "X_train = np.array(df_train.drop(['Survived'],axis = 1))\n",
    "y_train = np.array(df_train['Survived'])\n",
    "\n",
    "test_loc = r\"C:\\Users\\varun\\Coding\\ML\\titanic\\test.csv\"\n",
    "df_test = Dataprocess(test_loc)\n",
    "X_test = np.array(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (891, 7)\n",
      "Y_train:  (891, 1)\n",
      "X_test:   (418, 7)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "print('X_train: ' ,(X_train.shape))\n",
    "print('Y_train: ' ,(y_train.shape))\n",
    "print('X_test:  ' ,(X_test.shape))\n",
    "\n",
    "X_train = X_train.T\n",
    "y_train = y_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z) :\n",
    "    Z = np.asarray(Z, dtype = np.float64)\n",
    "    return(1/(1+np.exp(-Z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):  \n",
    "\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y,A):\n",
    "    cost = -np.mean(np.multiply(y,np.log(A)) + np.multiply((1-y),np.log(1-A)))\n",
    "    cost = float(np.squeeze(cost))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters) :\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    Z1 = cache[\"Z1\"]\n",
    "    Z2 = cache[\"Z2\"]\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2,A1.T)\n",
    "    db2 = (1/m) *(np.sum(dZ2,axis=1,keepdims=True))\n",
    "    dZ1 = np.dot(W2.T,dZ2) * (1 - np.power(A1,2))\n",
    "    dW1 = (1/m) *(np.dot(dZ1,X.T))\n",
    "    db1 = (1/m) *(np.sum(dZ1, axis=1, keepdims=True))\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "             \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693221\n",
      "Cost after iteration 1000: 0.461211\n",
      "Cost after iteration 2000: 0.438384\n",
      "Cost after iteration 3000: 0.423685\n",
      "Cost after iteration 4000: 0.414045\n",
      "Cost after iteration 5000: 0.410560\n",
      "Cost after iteration 6000: 0.408505\n",
      "Cost after iteration 7000: 0.406437\n",
      "Cost after iteration 8000: 0.404341\n",
      "Cost after iteration 9000: 0.402446\n"
     ]
    }
   ],
   "source": [
    "n_x = 7\n",
    "n_h = 4\n",
    "n_y = 1\n",
    "\n",
    "num_iterations = 10000\n",
    "learning_rate = 0.05\n",
    "# Initialize parameters\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]\n",
    "\n",
    "# Loop (gradient descent)\n",
    "for i in range(0, num_iterations):\n",
    "    # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\"\n",
    "    cache = forward_propagation(X_train, parameters)\n",
    "    # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\"\n",
    "    cost = cost_function(y_train, cache[\"A2\"])\n",
    "    # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\"\n",
    "    grads = backward_propagation(parameters, cache, X_train, y_train)\n",
    "    # Update rule for each parameter\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    # If print_cost=True, Print the cost every 1000 iterations\n",
    "    if i % 1000 == 0:\n",
    "        print (\"Cost after iteration %i: %f\" %(i, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.94051627384961\n"
     ]
    }
   ],
   "source": [
    "result = forward_propagation(X_train,parameters)\n",
    "final = result[\"A2\"]\n",
    "final[final > 0.5] = 1\n",
    "final[final < 0.5] = 0\n",
    "\n",
    "print(np.mean(final == y_train)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=iterations\n",
    "y=cost\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title(\"Logistic Regression\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ccf9f24fb63ed59c236fd10a93cb8b545ca3a39e47263b99763c8981aafa253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
